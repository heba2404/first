{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heba2404/first/blob/main/static_files/assignments/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "sWhgEFHuEJQr"
      },
      "source": [
        "# Devoir 3 &mdash; Images Panoramiques\n",
        "**Échéance : 06 avril 2025 &ndash; 23h59**\n",
        "<br><br>\n",
        "\n",
        "## Récupération des fichiers dans Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o-_scU5xEJQw"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Exécutez cette cellule pour préparer votre environnement Colab.\n",
        "#\n",
        "%%capture\n",
        "!wget https://benhadid.github.io/m1vpo/static_files/assignments/hw2.zip\n",
        "!unzip hw2.zip\n",
        "!mv hw2/* .\n",
        "!rm -rf hw2\n",
        "!rm -rf hw2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7XfLbhnEJQz"
      },
      "source": [
        "## Initialisation du bloc-notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eC2z0qdPEJQ0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# Setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15, 12) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# configurer Numpy pour afficher trois decimals après la virgule\n",
        "#np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "# pour le rechargement automatique des modules externes\n",
        "%load_ext autoreload\n",
        "%autoreload\n",
        "\n",
        "## données références pour validation des TODOs\n",
        "loaded = np.load('./arrays.npz', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "en",
        "id": "5u4GWtMMEJQ2"
      },
      "source": [
        "## 1. Description\n",
        "\n",
        "L'assemblage d'images en panorama est un très grand succès de la Vision Par Ordinateur. Matthew Brown et David G. Lowe ont publié un célèbre [article](https://drive.google.com/file/d/1qB54hR4TS_7But2KKkvuk6jTV1uWg8Tf/view?usp=sharing) sur cette technique en 2007. Depuis lors, cette technologie a été largement adoptée dans de nombreuses applications telles que \"**Google Street View**\" et les prises d'images panoramiques sur les smartphones.\n",
        "\n",
        "Dans ce devoir, vous allez implémenter une solution pour combiner une série d'images se chevauchant en une seule image panoramique. La détection des points-clés dans les images et un appariement initial est déjà fourni dans le code de démarrage (usage de fonctions ORB de OpenCV).\n",
        "\n",
        "Dans un premier temps, vous implémenterez le code qui calcule la matrice de transformation homographique entre deux images à partir de paires de correnspondance de points-clés. Ensuite, vous coderez une fonction pour séparer les bonnes correspondances (inliers) des mauvaises (outliers) selon le principe de l'algorithme [RANdom SAmple Consensus - RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus). Cela permetra l'alignement automatiquement des images de façpn robuste (positions relatives les unes par rapport aux autres ainsi que le calcul des zones de chevauchements de celles-ci). En troisième étape, afin d'appliquer les opérations de projections des images en un panorama, vous coderez l'algorithme de projection bilineaire inverse discuté en [cours](). Enfin, les images résultantes devraient être fusionnées en un seul panorama homogène (suppression des effets de bord dans l'image finale).\n",
        "\n",
        "Pour résumer, les étapes requises pour créer un panorama sont enumérées ci-dessous. Celles que vous devez implémenter dans ce devoir sont indiquées en gras :\n",
        "\n",
        "1. Extraire les primitives dans les images\n",
        "2. Mise en correspondance des primitives\n",
        "3. **Calcul de la matrice de transformation**\n",
        "4. **Alignement des images à l'aide de RANSAC**\n",
        "5. **Projection et interpolation des images**\n",
        "4. **Fusion des images en une image panoramique**\n",
        "\n",
        "## 2. Règles de codage\n",
        "\n",
        "**<span style='color:Red'>\n",
        "NE MODIFIEZ PAS LE CODE SQUELETTE EN DEHORS DES BLOCS TODO.<br>L'EVALUATEUR AUTOMATIQUE SERA TRES MECHANT AVEC VOUS SI VOUS LE FAITES !\n",
        "</span>**\n",
        "\n",
        "### 2.1. Résumé des fonctions potentiellement utiles (vous n'êtes pas obligé de les utiliser)  \n",
        "- numpy.divide,\n",
        "- numpy.eye,\n",
        "- numpy.ndarray,\n",
        "- numpy.dot,\n",
        "- numpy.linalg.svd,\n",
        "- numpy.linalg.lstsq,\n",
        "- numpy.linalg.inv,\n",
        "- numpy.meshgrid\n",
        "\n",
        "### 2.2. Résumé des fonctions <span style='color:Red'>interdites</span>\n",
        "- cv2.findHomography,\n",
        "- cv2.perspectiveTransform,\n",
        "- cv2.warpPerspective,\n",
        "- cv2.remap,\n",
        "- cv2.getAffineTransform,\n",
        "- cv2.getPerspectiveTransform,\n",
        "- cv2.invertAffineTransform,\n",
        "- cv2.warpAffine,\n",
        "- skimage.transform.ProjectiveTransform,\n",
        "- skimage.measure.ransac,\n",
        "- skimage.transform.SimilarityTransform,\n",
        "- skimage.transform.AffineTransform,\n",
        "- skimage.transform.FundamentalMatrixTransform,\n",
        "- skimage.transform.warp,\n",
        "- skimage.transform.warp_coords\n",
        "\n",
        "Vous pouvez utiliser ces fonctions pour le débogage de votre code, mais la version finale ne doit en aucun cas les inclure faute d'avoir un zéro pour le devoir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0SLqpFNEJQ3"
      },
      "source": [
        "## 3. Préparation des données (détection des points-clés et appariemment initial)\n",
        "\n",
        "Dans la cellule suivante, nous exécutons un algorithme de détéction de coins sur deux images, et récupérons les descripteurs associés à ces clés. Ensuite, nous effectuons une mise en correspondance entre les deux images et affichons l'ensemble des résultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWhpbvSUEJQ4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from utils import plot_matches\n",
        "\n",
        "# Commençant par charger deux images\n",
        "img1 = cv2.imread('resources/al_ain/Image1.jpg')\n",
        "img2 = cv2.imread('resources/al_ain/Image2.jpg')\n",
        "\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "imgs = [img1, img2]\n",
        "\n",
        "# construisons l'objet orb pour la detection, description\n",
        "orb = cv2.ORB_create()\n",
        "\n",
        "# détection des points-clés et construction des\n",
        "# descripteurs associés\n",
        "keypoints_list   = []  # keypoints[i] corresponds to imgs[i]\n",
        "descriptors_list = []  # descriptors[i] corresponds to keypoints[i]\n",
        "for img in imgs:\n",
        "    # trouver les points-clés et leurs descripteurs\n",
        "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
        "\n",
        "    keypoints_list.append(keypoints)\n",
        "    descriptors_list.append(descriptors)\n",
        "\n",
        "# construction de l'objet BFMatcher pour la mise en correspondance\n",
        "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "# matches_list[i] décrit la paire de correspondance entre les descripteurs descriptors[i] et descriptors[i+1]\n",
        "matches_list = []\n",
        "for pair in zip(descriptors_list[:-1], descriptors_list[1:]):\n",
        "    matches = bf.match(pair[0], pair[1])\n",
        "    matches_list.append(matches)\n",
        "\n",
        "# Affichage des points-clés détectés pour les deux premières images\n",
        "cvKeypoints1 = keypoints_list[0]\n",
        "cvKeypoints2 = keypoints_list[1]\n",
        "\n",
        "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 12))\n",
        "\n",
        "tmp = cv2.drawKeypoints(img1, cvKeypoints1, None, color=(0,0,255), flags=0)\n",
        "ax1.imshow(tmp)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Points-clés détectés dans Image 1', y=-0.10)\n",
        "\n",
        "tmp = cv2.drawKeypoints(img2, cvKeypoints2, None, color=(0,0,255), flags=0)\n",
        "ax2.imshow(tmp)\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Points-clés détectés dans Image 2', y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "# traçons les correspondances...\n",
        "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
        "ax.axis('off')\n",
        "\n",
        "matches = np.array([ (m.queryIdx, m.trainIdx) for m in matches_list[0] ])\n",
        "\n",
        "plot_matches(ax, img1, img2, cvKeypoints1, cvKeypoints2, matches,\n",
        "                 keypoints_color='k', matches_color=None, only_matches=False)\n",
        "plt.title('Paires de correspondance détectées (avec \\'outliers\\')', y=-0.10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "J_aaCOg7EJQ5"
      },
      "source": [
        "## 4. Les tâches\n",
        "\n",
        "### 4.1. Estimation de la matrice de transformation [TODO 1] (10 points)\n",
        "\n",
        "Nous avons maintenant une liste de points clés correspondants sur les deux images. Nous allons l'utiliser pour trouver une matrice de transformation qui mappe les points de la première image aux coordonnées correspondantes dans la deuxième image. En d'autres termes, si le point $p_1 = \\left(x_1,y_1\\right)$ dans l'image 1 correspond à $p_2=\\left(x_2,y_2\\right)$ dans l'image 2, nous devons trouver une matrice de transformation $\\mathbf{H}$ telle que :\n",
        "\n",
        "$$\n",
        "\\tilde{p_1}\\mathbf{H}^T = \\tilde{p_2},\n",
        "$$\n",
        "\n",
        "où $\\tilde{p_1}$ et $\\tilde{p_2}$ sont les coordonnées homogènes associées à $p_1$ et $p_2$.\n",
        "\n",
        "Notez qu'il peut être impossible de trouver la transformation $\\mathbf{H}$ qui mappe chaque point de l'image 1 **exactement** au point correspondant de l'image 2. Cependant, nous pouvons estimer la matrice de transformation avec les moindres carrés ou la décomposition en valeurs singulières. Étant donné $N$ paires de points-clés correspondants, soit $\\mathbf{X_1}$ et $\\mathbf{X_2}$ des matrices $ N \\times 3 $ dont les lignes sont des coordonnées homogènes des points clés correspondants dans l'image 1 et l'image 2 respectivement. Nous pouvons estimer $\\mathbf{H}$ en résolvant le problème des moindres carrés,\n",
        "\n",
        "$$\n",
        "\\mathbf{X_1}\\,\\mathbf{H}^T = \\mathbf{X_2}\n",
        "$$\n",
        "\n",
        "Dans la fonction **`fit_transform_matrix()`** du fichier` panorama.py`, implémentez le code pour estimer la matrice de transformation $\\mathbf{H}$ à partir des coordonnées des points-clés appariés $p_1$ et $p_2$.\n",
        "\n",
        "- **Indication** : Consultez les slides n° 12 et 40 du cours \"[alignement des images](https://benhadid.github.io/m1vpo/lectures/)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CnLK-34EJQ7"
      },
      "outputs": [],
      "source": [
        "from panorama import fit_transform_matrix\n",
        "\n",
        "#\n",
        "# testq de validité pour fit_transform_matrix()\n",
        "#\n",
        "\n",
        "# première image -- coordonnées organisées en ( y , x )\n",
        "p0 = np.array([[0.0, 0.0],       # coin supérieur gauche\n",
        "               [0.0, 1.0],       # coin supérieur droit\n",
        "               [1.0, 0.0],       # coin inférieur gauche\n",
        "               [1.0, 1.0]])      # coin inférieur droit\n",
        "\n",
        "# deuxième image (première image pivotée de 45°)\n",
        "v  = np.sqrt(2)/2.\n",
        "p1 = np.array([[ 0.0, 0.0],\n",
        "               [   v,   v],\n",
        "               [   v,  -v],\n",
        "               [ v+v, 0.0]])\n",
        "\n",
        "#troisième image (première image agrandie deux fois horizontalement)\n",
        "p2 = np.array([[ 0.0, 0.0],\n",
        "               [ 0.0, 2.0],\n",
        "               [ 1.0, 0.0],\n",
        "               [ 1.0, 2.0]])\n",
        "\n",
        "#quatrième image (première image projetée par Homographie)\n",
        "p3 = np.array([[ 4.3,   7.3],\n",
        "               [ 8.2,   7.2],\n",
        "               [ 4.6,  10.7],\n",
        "               [ 9.2,   9.2]])\n",
        "\n",
        "# premier test\n",
        "H1 = fit_transform_matrix(p0[:,[1,0]], p0[:,[1,0]])    #( [:, [1,0] ] <-> permutation des colonnes y et x)\n",
        "H1 = H1 / H1[2,2]\n",
        "\n",
        "# sortie attendue : la matrice identité\n",
        "sol1 = np.eye(3)\n",
        "\n",
        "#second test\n",
        "H2 = fit_transform_matrix(p0[:,[1,0]], p1[:,[1,0]])\n",
        "H2 = H2 / H2[2,2]\n",
        "\n",
        "# sortie attendue : matrice de rotation (45°)\n",
        "sol2 = np.array([[ v, -v, 0],\n",
        "                 [ v,  v, 0],\n",
        "                 [ 0,  0, 1]])\n",
        "\n",
        "#troisième test\n",
        "H3 = fit_transform_matrix(p0[:,[1,0]], p2[:,[1,0]])\n",
        "H3 = H3 / H3[2,2]\n",
        "\n",
        "# sortie attendue : matrice de changement d’échelle np.diag(2, 1, 1)\n",
        "sol3 = np.array([[ 2.0,  0.0, 0],\n",
        "                 [ 0.0,  1.0, 0],\n",
        "                 [ 0.0,  0.0, 1]])\n",
        "\n",
        "#quatrième test\n",
        "H4 = fit_transform_matrix(p0[:,[1,0]], p3[:,[1,0]])\n",
        "H4 = H4 / H4[2,2]\n",
        "\n",
        "# sortie attendue : matrice Homographie standard (non-affine)\n",
        "sol4 = np.array([[ 3.52692,  0.6,      7.3    ],\n",
        "                 [ 8.03065, -0.90374,  4.3    ],\n",
        "                 [ 0.50374, -0.26168,  1.     ]])\n",
        "\n",
        "# comparaison des résultats\n",
        "if  np.allclose(sol1, H1, rtol=1e-05, atol=1e-05):\n",
        "    print('test identité   :  correct ! ')\n",
        "else:\n",
        "    print('test identité   :  incorrect !\\nH = %s, \\n\\nsol = %s\\n' % (H1,sol1))\n",
        "\n",
        "if  np.allclose(sol2, H2, rtol=1e-05, atol=1e-05):\n",
        "    print('test rotation   :  correct ! ')\n",
        "else:\n",
        "    print('test rotation   :  incorrect !\\nH = %s, \\n\\nsol = %s\\n' % (H2,sol2))\n",
        "\n",
        "if  np.allclose(sol3, H3, rtol=1e-05, atol=1e-05):\n",
        "    print('test échelle    :  correct ! ')\n",
        "else:\n",
        "    print('test échelle    :  incorrect !\\nH = %s, \\n\\nsol = %s\\n' % (H3,sol3))\n",
        "\n",
        "if  np.allclose(sol4, H4, rtol=1e-05, atol=1e-05):\n",
        "    print('test projection :  correct ! ')\n",
        "else:\n",
        "    print('test projection :  incorrect !\\nH = %s, \\n\\nsol = %s\\n' % (H4,sol4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "bHZVDOzoEJQ-"
      },
      "source": [
        "### 4.2. Alignement des images à l'aide de RANSAC [TODO 2] (20 points)\n",
        "Au lieu d'utiliser directement toutes nos correspondances de points-clés comme entrée pour la fonction `fit _transform_ matrix()`, il est préférable de sélectionner uniquement les points-clés valides (c.-à-d. les 'inliers') pour calculer la matrice de transformation $\\mathbf{H}$. Nous utiliserons l'algorithme RANSAC ([RANdom SAmple Consensus](https://en.wikipedia.org/wiki/Random_sample_consensus)) pour cette tâche. Cet algorithme est modelé selon une approche probabiliste qui permet de classer les correnspondances entre points-clés en deux catégories : Les bonnes correspondances (inliers), et les mauvaises correspondances (outliers). Les étapes dans RANSAC sont énumérés comme suit :\n",
        "1. Sélectionner, d'une manière aléatoire, un sous-ensemble de points-clés (c.-à-d. quatre paires) dans l'ensemble des correspondances $C$.   \n",
        "2. Calculer une matrice de transformation $\\hat{\\mathbf{H}}$ à partir du sous-ensemble sélectionné.\n",
        "3. Appliquer la matrice $\\hat{\\mathbf{H}}$ à l'ensemble complémentaire $\\bar{C}$ et determiner les bonnes correspondances (inliers) selon un seuil prédéfini.\n",
        "4. Répéter les étapes 1&ndash;3 plusieurs fois et conserver la matrice  $\\hat{\\mathbf{H}}$ qui produit le plus grand nombre de bonnes correspondances avec une confidence $\\geq 99.9\\%$.\n",
        "5. Recalculer une meilleure estimation de la matrice $\\hat{\\mathbf{H}}$ en utilisant que les paires de points-clés classées comme &laquo; bonnes correspondances &raquo; (c.-à-d. les 'inliers').\n",
        "\n",
        "Implémentez la fonction **`ransac()`** dans` panorama.py`. Puis, exécutez le code suivant pour validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WldNLhuLEJQ_"
      },
      "outputs": [],
      "source": [
        "from panorama import ransac\n",
        "\n",
        "# récupérer les coordonnées des points clés depuis les structures opencv\n",
        "keypoints1 = np.squeeze( np.array( [  kp.pt  for kp in cvKeypoints1 ] ) )\n",
        "keypoints2 = np.squeeze( np.array( [  kp.pt  for kp in cvKeypoints2 ] ) )\n",
        "\n",
        "# lancer RANSAC\n",
        "H, robust_matches = ransac(keypoints1, keypoints2, matches)\n",
        "\n",
        "# Visualisation des correspondances robustes retournées par RANSAC\n",
        "_, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
        "plot_matches(ax, img1, img2, cvKeypoints1, cvKeypoints2, robust_matches)\n",
        "plt.axis('off')\n",
        "plt.title('RANSAC (résultat)', y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('ransac.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.title('RANSAC (référence)', y=-0.10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "12JutlBHEJRA"
      },
      "source": [
        "### 4.3. Projection inverse des images et interpolation bilineaire [TODO3] (40 points)\n",
        "\n",
        "Nous pouvons maintenant utiliser la matrice de transformation $\\mathbf{H}$ calculée à l'aide de RANSAC pour transformer nos images et créer un panorama ! En ce sens, nous avons besoin d'implémenter une méthode qui projette les coordonnées de chaque pixel dans l'image source vers les coordonnées transformées dans l'image destination.\n",
        "\n",
        "Dans ce devoir, la fonction **`warp_image()`** dans `panorama.py` doit réaliser l'étape de \"projection inverse avec interpolation bilinéaire\" présentée dans le cours \"[Images panoramiques](https://benhadid.github.io/m1vpo/lectures/)\". C-à-d. Pour transformer les pixels de l'image source, nous commencerons à partir des coordonnées de l'image destination et nous utiliserons l'interpolation bilinéaire dans l'image source pour calculer les couleurs des pixels à reproduire dans l'image destination.\n",
        "\n",
        "Une fois la fonction **`warp_image()`** implémentée, exécutez le code ci-dessous pour effectuer l'étape de projection.\n",
        "\n",
        "**Indications :**\n",
        "- Lorsque vous manipulez des coordonnées homogènes, n'oubliez pas de les normaliser avant de les reconvertir en coordonnées cartésiennes.\n",
        "- Attention aux points en dehors de l'image source lors de la projection inverse. Ils ne doivent pas être inclus dans les calculs de l'image cible.\n",
        "- Commencez par travailler sur le code en bouclant sur chaque pixel d'une manière itérative (approche classique). Ensuite, vous pouvez optimiser votre code en utilisant des instructions Numpy/Scipy (pensez à utiliser `numpy.meshgrid`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok99ZcwJEJRB"
      },
      "outputs": [],
      "source": [
        "from panorama import get_output_space, warp_image\n",
        "\n",
        "#left on right\n",
        "transforms = [H , np.eye(3) ]\n",
        "\n",
        "#right on left\n",
        "#transforms = [np.eye(3), np.linalg.inv(H)  ]\n",
        "\n",
        "# 'get_output_space' est une fonction auxiliaire qui aide\n",
        "# à trouver le cadre global associé à une liste d'images transformées\n",
        "output_shape, offset = get_output_space(imgs, transforms)\n",
        "\n",
        "# transformation des images\n",
        "img1_warped, _ = warp_image(imgs[0], transforms[0], output_shape, offset)\n",
        "img2_warped, _ = warp_image(imgs[1], transforms[1], output_shape, offset)\n",
        "\n",
        "# Visualisation des images transformées\n",
        "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 12))\n",
        "ax1.imshow(img1_warped[:,:,:3])\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Image 1 projetée (résultat)',y=-0.10)\n",
        "\n",
        "ax2.imshow(img2_warped[:,:,:3])\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Image 2 projetée (résultat)',y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('warped.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "XJASuAc5EJRC"
      },
      "source": [
        "### 4.4. Assemblage (naïve) des images [TODO 4] (5 points)\n",
        "\n",
        "Une fois les images transformées, il reste à les assembler en une seule image panoramique. Une méthode simple (et naïve) est d'additionner les deux images. Ainsi, pour chaque pixel $\\left[i,j\\right]$ dans l'image de destination, nous avons :\n",
        "\n",
        "$$\n",
        "dst[i,j] = \\left(src_1\\left[i,j\\right] + src_2\\left[i,j\\right]\\right) \\bigg/ \\sum_{k=1}^{2} alpha_k\\left[i,j\\right]\n",
        "$$\n",
        "\n",
        "où\n",
        "- $src_1[i,j]$ et $src_2[i,j]$ sont respectivement les couleurs des pixels dans les images $src_1$ et $src_2$.\n",
        "- $alpha_1$ et $alpha_2$ sont les canaux de transparence associés à ces images.\n",
        "\n",
        "Implémentez la fonction **``naive_fusion()``** dans `panorama.py` puis exécutez le code suivant pour validation. Le panorama obtenu présentera sans doute des artéfacts de bords, mais nous verrons plus tard comment obtenir un meilleur résultat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7kgJfeGEJRC"
      },
      "outputs": [],
      "source": [
        "from panorama import naive_fusion\n",
        "\n",
        "src1_warped = loaded['img1_warped']\n",
        "src2_warped = loaded['img2_warped']\n",
        "\n",
        "#décommentez les deux lignes suivantes pour tester la fonction naive_fusion() sur vos images (au lieu des images références).\n",
        "#src1_warped = img1_warped\n",
        "#src2_warped = img2_warped\n",
        "\n",
        "merged = naive_fusion(src1_warped, src2_warped)\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(merged)\n",
        "plt.axis('off')\n",
        "plt.title(\"Fusion d'images - alpha moyen (résultat)\", y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('merged.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.title(\"Fusion d'images - alpha moyen (référence)\", y=-0.10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "tV_li1r0EJRC"
      },
      "source": [
        "### 4.5. Cartes de pondération [TODO 5] (20 points)\n",
        "Vous avez sans doute remarqué les filons désagréables au milieu de votre image panoramique. Il est possible de lisser ces artefacts et produire une image plus agréable à l'oeil en utilisant une technique très simple appelée \"contour progressif\" (*anglais* : feathering). Actuellement, tous les pixels de la région de chevauchement des images sont pondérés de manière égale (c.-à-d. le canal alpha de l'image = 0.5). Cependant, comme les pixels aux extrémités de la zone de chevauchement sont très bien complétés par les pixels de l'autre image, nous pouvons faire en sorte qu'ils contribuent moins au signal dans le panorama final.\n",
        "\n",
        "<img src=\"https://github.com/benhadid/m1vpo/blob/gh-pages/static_files/assignments/figures/feathering.png?raw=1\" width=\"45%\"/>\n",
        "\n",
        "Le mélange par \"contour progressif\" peut être appliqué aux images en trois étapes :\n",
        "1. Calcul d'une carte de pondération pour chaque image à fusionner : Pondération de chaque pixel de l'image source proportionnellement à sa distance du bord. Les pixels au milieu de l'image ont un poids plus important par rapport aux pixels aux bords de l'image.  \n",
        "2. Application des cartes de pondérations aux images correspondantes\n",
        "3. Pour chaque pixel dans l’image fusionnée finale, division de la valeur du pixel (c.-à-d. la couleur)\n",
        "   par la sommes des coefficients de pondération à cet pixel.\n",
        "\n",
        "En ce sens, réécrivez la fonction **`warp_image`** dans `panorama.py` pour qu'elle calcule, en plus de la transformation initialement implémentée, la carte de pondération des alphas de l'image transformée selon la technique du contour progressif. Le calcul des coefficients de pondération doit être contrôlé par un paramètre supplémentaire appelé `method` où :\n",
        " - method = None     -- aucun changement. La fonction retourne des images avec le canal alpha égale à 1.0 (TODO4).\n",
        " - method ='hlinear' -- calcul des coefficients de pondération de l'image transformée dans le sens horizontal seulement\n",
        " - method ='vlinear' -- calcul des coefficients de pondération de l'image transformée dans le sens vertical seulement\n",
        " - method ='linear'  -- calcul des coefficients de pondération de l'image transformée dans le sens horizontal et vertical.\n",
        "\n",
        "Exécutez le code suivant pour visualiser les cartes de pondération obtenues à l'aide de la technique du \"contour progressif\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kJs6yMuEEJRD"
      },
      "outputs": [],
      "source": [
        "limg1_warped, m1 = warp_image(imgs[0], transforms[0], output_shape, offset, method='linear')\n",
        "limg2_warped, m2 = warp_image(imgs[1], transforms[1], output_shape, offset, method='linear')\n",
        "\n",
        "# Visualisation des cartes de pondérations\n",
        "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 12))\n",
        "ax1.imshow(limg1_warped[:,:,3], cmap='gray')\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Carte de pondération de Image 1 (résultat)',y=-0.10)\n",
        "\n",
        "ax2.imshow(limg2_warped[:,:,3], cmap='gray')\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Carte de pondération de Image 2 (résultat)',y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('alpha.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyR-c_yMEJRE"
      },
      "source": [
        "### 4.6. Assemblage avec pondération des images [TODO 6] (5 points)\n",
        "\n",
        "Maintenant que nous ayons des cartes de pondération plus adaptées, nous pouvons fusioner les images transformées `img1_warped` et `img2_warped`  selon la formule suivante :\n",
        "\n",
        "$$\n",
        "dst[i,j] = \\left( alpha_1\\left[i,j\\right] \\times src_1\\left[i,j\\right] + alpha_2\\left[i,j\\right] \\times src_2\\left[i,j\\right]\\right) \\bigg/ \\sum_{k=1}^{2} alpha_k\\left[i,j\\right]\n",
        "$$\n",
        "\n",
        "Implémentez la fonction **``fusion()``** dans `panorama.py` qui réalise cette opération, puis exécutez le code ci-dessous pour validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GnmiJqhEJRE"
      },
      "outputs": [],
      "source": [
        "from panorama import fusion\n",
        "\n",
        "src1_warped = loaded['limg1_warped']\n",
        "src2_warped = loaded['limg2_warped']\n",
        "lm1         = loaded['m1']\n",
        "lm2         = loaded['m2']\n",
        "\n",
        "#décommentez les quatre lignes suivantes pour tester la fonction fusion() sur vos données (au lieu des données références).\n",
        "#src1_warped = limg1_warped\n",
        "#src2_warped = limg2_warped\n",
        "#lm1 = m1\n",
        "#lm2 = m2\n",
        "\n",
        "merged = fusion(src1_warped, lm1, src2_warped, lm2)\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(merged)\n",
        "plt.axis('off')\n",
        "plt.title(\"fusion avec lissage des bordures (résultat)\", y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('fused.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.title(\"fusion avec lissage de bordure (référence)\", y=-0.10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "fr",
        "id": "IswMniNdEJRF"
      },
      "source": [
        "### 5. Fusion d'images multiples dans un panorama [TODO  Bonus] (20 points)\n",
        "\n",
        "Étant donné une séquence ordonnée d'images ($I_1, I_2,...,I_m,\\text{ où } m \\geq 2$), nous aimerions produire un panorama à partir de cette séquence. En ce sens, implémentez la fonction **`stitch_multiple_images()`** dans `panorama.py` qui réalise cette opération.\n",
        "\n",
        "\n",
        "**Indications :**\n",
        "\n",
        "- Pensez à réutiliser les fonctions que vous avez déjà implémentées dans `panorama.py`.\n",
        "- Les transformations des images $I_{i}$ pour produire l'image panoramique finale doivent être référencées par rapport à l'image $I_{ref}$ &ndash; C.-à-d. le 'milieu' de votre image panoramique est indiqué par le paramètre `imgref` de la fonction `stitch_multiple_images()`.\n",
        "  - Prenez chaque paire d'images voisine et calculez la matrice de transformation $\\mathbf{H}_i$ qui convertit les coordonnées des pixels dans l'image $I_{i}$ aux nouvelles coordonnées dans l'image $I_{i+1}$.\n",
        "  - Appliquez ensuite les opérations algébriques nécessaires pour réadapter ces transformations par rapport à l'image référence $I_{ref}$. Si vous êtes confus, vous pouvez revoir les slides du cours sur les transformations géométriques et comment les combinées pour effectuer une transformation complexe à partir d'une suite de transformations élémentaires.\n",
        "  - L'inverse de la matrice de transformation $\\mathbf{H}$ a l'effet inverse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VdvrdcFhEJRF"
      },
      "outputs": [],
      "source": [
        "from panorama import stitch_multiple_images\n",
        "\n",
        "# Load images to stitch\n",
        "img1 = cv2.imread('resources/city/01.jpg')\n",
        "img2 = cv2.imread('resources/city/02.jpg')\n",
        "img3 = cv2.imread('resources/city/03.jpg')\n",
        "img4 = cv2.imread('resources/city/04.jpg')\n",
        "img5 = cv2.imread('resources/city/05.jpg')\n",
        "\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
        "img4 = cv2.cvtColor(img4, cv2.COLOR_BGR2RGB)\n",
        "img5 = cv2.cvtColor(img5, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "imgs = [img1, img2, img3, img4, img5]\n",
        "\n",
        "keypoints_list   = []  # keypoints[i] associated with imgs[i]\n",
        "descriptors_list = []  # descriptors[i] associated with keypoints[i]\n",
        "for img in imgs:\n",
        "    # find the keypoints and descriptors with ORB\n",
        "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
        "    keypoints = np.squeeze( np.array( [  kp.pt  for kp in keypoints ] ) )\n",
        "\n",
        "    keypoints_list.append(keypoints)\n",
        "    descriptors_list.append(descriptors)\n",
        "\n",
        "# matches_list[i] décrit la paire de correspondance entre les descripteurs descriptors[i] et descriptors[i+1]\n",
        "matches_list = []\n",
        "for pair in zip(descriptors_list[:-1], descriptors_list[1:]):\n",
        "    # find the matches between the sets of descriptors (and thus keypoints)\n",
        "    matches = bf.match(pair[0], pair[1])\n",
        "    matches = np.array( [ (m.queryIdx, m.trainIdx) for m in matches ] )\n",
        "\n",
        "    matches_list.append( matches )\n",
        "\n",
        "# Stitch images together\n",
        "panorama = stitch_multiple_images(imgs, keypoints_list, matches_list, imgref=2, blend='linear')\n",
        "\n",
        "# Plot initial images\n",
        "_, ax = plt.subplots(1, len(imgs), figsize=(15, 12))\n",
        "for i, img in enumerate(imgs):\n",
        "    ax[i].imshow(img[:,:,:3])\n",
        "    ax[i].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# show panorama image\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(panorama)\n",
        "plt.axis('off')\n",
        "plt.title(\"Image panoramique (résultat)\", y=-0.10)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(1, 1, figsize=(15, 12))\n",
        "plt.imshow(cv2.cvtColor(cv2.imread('panorama.png'), cv2.COLOR_BGR2GRAY))\n",
        "plt.axis('off')\n",
        "plt.title(\"Image panoramique (référence)\", y=-0.10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZskhXjQaEJRG"
      },
      "source": [
        "## 5. Livrables\n",
        "\n",
        "### 5.1. Le code (à remettre sur [benhadid.ddns.net](https://benhadid.ddns.net/course/M1_VPO/hw2))\n",
        "\n",
        "Le fichier \"devoir3.zip\" contenant le fichier **`panorama.py`** modifié dans les zones indiquées par `#TODO-BLOC-DEBUT` et `#TODO-BLOC-FIN`.\n",
        "\n",
        "**Le code sera remis <del>en classe pendant votre séance de TP</del> au serveur INGInious - <span style='color:Red'> aucun document ou code ne sera accepté si envoyé par mail ou clé USB</span>**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE8bTQ-aEJRG"
      },
      "outputs": [],
      "source": [
        "# Exécutez cette cellule puis récupérez votre fichier \"devoir3.zip\" depuis le panneau gauche dans Colab\n",
        "# Si nécessaire, faites un 'refresh' sur le répertoire pour faire apparaître le fichier 'devoir3.zip'\n",
        "\n",
        "!zip -r ./devoir3.zip panorama.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}